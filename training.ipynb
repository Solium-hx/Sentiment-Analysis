{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "import re\n",
    "\n",
    "import Mic.video as mic\n",
    "import Camera.video as cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_path = \"./dataset/IEMOCAP_features.pkl\"\n",
    "iemocap_features = pickle.load(open(feature_path, 'rb'), encoding='latin1')\n",
    "\n",
    "videoIDs, videoSpeakers, videoLabels, videoText, videoAudio, videoVisual, videoSentence, trainVid, testVid = iemocap_features\n",
    "emotionIDs = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru', 'fer', 'sup', 'dis']\n",
    "\n",
    "\n",
    "audio_emotions = {\n",
    "    'Statement': None,\n",
    "    'Text': {\n",
    "        'Emotion': None,\n",
    "        'Score': None,\n",
    "    },\n",
    "    'Audio': {\n",
    "        'Emotion': None,\n",
    "        'Score': None,\n",
    "    }\n",
    "}\n",
    "\n",
    "video_emotions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time(duration):\n",
    "    return [float(time) for time in re.findall(\"\\d+\\.\\d+\", duration)]\n",
    "\n",
    "def create_clips(vid_path, start_time, end_time):\n",
    "        ffmpeg_extract_subclip(vid_path, start_time, end_time, targetname=\"train_clip.avi\")\n",
    "\n",
    "        clip = VideoFileClip(\"train_clip.avi\")\n",
    "        width = clip.w\n",
    "        clip.crop(x1=0, width=width // 2).write_videofile(\"left_half.mp4\")\n",
    "        clip.crop(x1=width // 2, width=width // 2).write_videofile(\"right_half.mp4\")\n",
    "\n",
    "def get_clip_speaker(vid_seg_id):\n",
    "    right_speaker = vid_seg_id[5]\n",
    "    if vid_seg_id.find('script') != -1:\n",
    "        speaker = vid_seg_id[18]\n",
    "    else:\n",
    "        speaker = vid_seg_id[15]\n",
    "\n",
    "    if speaker == right_speaker:\n",
    "        # use right_half\n",
    "        return \"right_half.mp4\"\n",
    "    \n",
    "    return \"left_half.mp4\"\n",
    "\n",
    "def extract_video(video_emotions):\n",
    "    EMOTIONS_LIST = [\"Angry\", \"Disgust\",\n",
    "                     \"Fear\", \"Happy\",\n",
    "                     \"Neutral\", \"Sad\",\n",
    "                     \"Surprise\"]\n",
    "    count = [video_emotions.count(emotion) for emotion in EMOTIONS_LIST]\n",
    "    mode = max(count)\n",
    "    if mode == 0:\n",
    "        mode = 1\n",
    "    return [num // mode for num in count]\n",
    "\n",
    "def extract_audio(audio_emotions):\n",
    "    EMOTIONS_LIST = ['neu', 'ang', 'hap', 'sad']\n",
    "    audio_input = [0] * len(EMOTIONS_LIST)\n",
    "    emotion = audio_emotions['Audio']['Emotion']\n",
    "    audio_input[EMOTIONS_LIST.index(emotion)] = 1\n",
    "    return audio_input\n",
    "\n",
    "def extract_text(audio_emotions):\n",
    "    EMOTIONS_LIST = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "    text_input = [0] * len(EMOTIONS_LIST)\n",
    "    emotion = audio_emotions['Text']['Emotion']\n",
    "    text_input[EMOTIONS_LIST.index(emotion)] = 1\n",
    "    return text_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for vid_id in trainVid:\n",
    "    ses_id = vid_id[4]\n",
    "    ses_path = f'./dataset/IEMOCAP_full_release/Session{ses_id}/dialog/'\n",
    "    vid_path = ses_path + 'avi/DivX/' + vid_id + '.avi'\n",
    "    eval_path = ses_path + 'EmoEvaluation/' + vid_id + '.txt'\n",
    "    \n",
    "\n",
    "    with open(eval_path, 'r') as eval_file:\n",
    "        eval_segments = [line for line in eval_file if line[0] == '[']\n",
    "        for segment in eval_segments:\n",
    "            duration, vid_seg_id, emotion, _ = segment.split('\\t')\n",
    "            print(vid_seg_id)\n",
    "            if emotion == 'xxx':\n",
    "                print(\"DNE\")\n",
    "                continue\n",
    "\n",
    "            start_time, end_time = extract_time(duration)\n",
    "            vid_len = end_time-start_time\n",
    "\n",
    "            create_clips(vid_path=vid_path, start_time=start_time, end_time=end_time)\n",
    "\n",
    "            video = get_clip_speaker(vid_seg_id=vid_seg_id)\n",
    "\n",
    "            print(\"Getting Audio\")\n",
    "            audio_file_success, audio_response = mic.get_audio(video, vid_len)\n",
    "            print(\"Done\")\n",
    "\n",
    "            if audio_file_success:\n",
    "                print(\"Running Text Analysis\")\n",
    "                text_analysis_success, text_response = mic.run_text_analysis(audio_response)\n",
    "                print(\"Done\")\n",
    "\n",
    "                if text_analysis_success:\n",
    "                    audio_emotions[\"Statement\"] = text_response[0]\n",
    "                    print(\"Statement: \", audio_emotions['Statement'])\n",
    "                    audio_emotions['Text'] = {\n",
    "                        'Emotion': text_response[1][0]['label'],\n",
    "                        'Score': text_response[1][0]['score']\n",
    "                    }\n",
    "                    print(\"Text: \", audio_emotions[\"Text\"])\n",
    "                \n",
    "                else:\n",
    "                    print(\"Error: \", text_response)\n",
    "                    audio_emotions = {\n",
    "                        'Statement': None,\n",
    "                        'Text': {\n",
    "                            'Emotion': None,\n",
    "                            'Score': None,\n",
    "                        },\n",
    "                        'Audio': {\n",
    "                            'Emotion': None,\n",
    "                            'Score': None,\n",
    "                        }\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "                print(\"Running Audio Analysis\")\n",
    "                out_prob, score, index, text_lab = mic.run_audio_analysis(audio_response)\n",
    "                audio_emotions['Audio'] = {\n",
    "                    'Emotion': text_lab[0],\n",
    "                    'Score': score.item()\n",
    "                }\n",
    "                print(\"Audio: \", audio_emotions[\"Audio\"])\n",
    "            \n",
    "            else:\n",
    "                print(\"Error: \", audio_response)\n",
    "                audio_emotions = {\n",
    "                    'Statement': None,\n",
    "                    'Text': {\n",
    "                        'Emotion': None,\n",
    "                        'Score': None,\n",
    "                    }\n",
    "                }\n",
    "                audio_emotions['Audio'] = {\n",
    "                    'Emotion': None,\n",
    "                    'Score': None,\n",
    "                }\n",
    "                break\n",
    "            \n",
    "            if audio_emotions['Statement'] != None:\n",
    "                print(\"Running Video Analysis\")\n",
    "                video_emotions = cam.get_pred_frame(video)\n",
    "                print(video_emotions)\n",
    "\n",
    "            video_input = extract_video(video_emotions)\n",
    "            audio_input = extract_audio(audio_emotions)\n",
    "            text_input = extract_text(audio_emotions)\n",
    "\n",
    "            input = [*video_input, *audio_input, *text_input]\n",
    "\n",
    "            expected_output = videoLabels[vid_id][videoIDs[vid_id].index(vid_seg_id)]\n",
    "\n",
    "            x_train.append(input)\n",
    "            y_train.append(expected_output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(150,150,150), max_iter=3000, verbose=1, random_state=21, tol=0.000000001)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "clf.fit(x_train, y_train)\n",
    "stop = timeit.default_timer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for vid_id in testVid:\n",
    "    ses_id = vid_id[4]\n",
    "    ses_path = f'./dataset/IEMOCAP_full_release/Session{ses_id}/dialog/'\n",
    "    vid_path = ses_path + 'avi/DivX/' + vid_id + '.avi'\n",
    "    eval_path = ses_path + 'EmoEvaluation/' + vid_id + '.txt'\n",
    "    \n",
    "\n",
    "    with open(eval_path, 'r') as eval_file:\n",
    "        eval_segments = [line for line in eval_file if line[0] == '[']\n",
    "        for segment in eval_segments:\n",
    "            duration, vid_seg_id, emotion, _ = segment.split('\\t')\n",
    "            print(vid_seg_id)\n",
    "            if emotion == 'xxx':\n",
    "                print(\"DNE\")\n",
    "                continue\n",
    "\n",
    "            start_time, end_time = extract_time(duration)\n",
    "            vid_len = end_time-start_time\n",
    "\n",
    "            create_clips(vid_path=vid_path, start_time=start_time, end_time=end_time)\n",
    "\n",
    "            video = get_clip_speaker(vid_seg_id=vid_seg_id)\n",
    "\n",
    "            print(\"Getting Audio\")\n",
    "            audio_file_success, audio_response = mic.get_audio(video, vid_len)\n",
    "            print(\"Done\")\n",
    "\n",
    "            if audio_file_success:\n",
    "                print(\"Running Text Analysis\")\n",
    "                text_analysis_success, text_response = mic.run_text_analysis(audio_response)\n",
    "                print(\"Done\")\n",
    "\n",
    "                if text_analysis_success:\n",
    "                    audio_emotions[\"Statement\"] = text_response[0]\n",
    "                    print(\"Statement: \", audio_emotions['Statement'])\n",
    "                    audio_emotions['Text'] = {\n",
    "                        'Emotion': text_response[1][0]['label'],\n",
    "                        'Score': text_response[1][0]['score']\n",
    "                    }\n",
    "                    print(\"Text: \", audio_emotions[\"Text\"])\n",
    "                \n",
    "                else:\n",
    "                    print(\"Error: \", text_response)\n",
    "                    audio_emotions = {\n",
    "                        'Statement': None,\n",
    "                        'Text': {\n",
    "                            'Emotion': None,\n",
    "                            'Score': None,\n",
    "                        },\n",
    "                        'Audio': {\n",
    "                            'Emotion': None,\n",
    "                            'Score': None,\n",
    "                        }\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "                print(\"Running Audio Analysis\")\n",
    "                out_prob, score, index, text_lab = mic.run_audio_analysis(audio_response)\n",
    "                audio_emotions['Audio'] = {\n",
    "                    'Emotion': text_lab[0],\n",
    "                    'Score': score.item()\n",
    "                }\n",
    "                print(\"Audio: \", audio_emotions[\"Audio\"])\n",
    "            \n",
    "            else:\n",
    "                print(\"Error: \", audio_response)\n",
    "                audio_emotions = {\n",
    "                    'Statement': None,\n",
    "                    'Text': {\n",
    "                        'Emotion': None,\n",
    "                        'Score': None,\n",
    "                    }\n",
    "                }\n",
    "                audio_emotions['Audio'] = {\n",
    "                    'Emotion': None,\n",
    "                    'Score': None,\n",
    "                }\n",
    "                break\n",
    "            \n",
    "            if audio_emotions['Statement'] != None:\n",
    "                print(\"Running Video Analysis\")\n",
    "                video_emotions = cam.get_pred_frame(video)\n",
    "                print(video_emotions)\n",
    "\n",
    "            video_input = extract_video(video_emotions)\n",
    "            audio_input = extract_audio(audio_emotions)\n",
    "            text_input = extract_text(audio_emotions)\n",
    "\n",
    "            input = [*video_input, *audio_input, *text_input]\n",
    "\n",
    "            expected_output = videoLabels[vid_id][videoIDs[vid_id].index(vid_seg_id)]\n",
    "\n",
    "            x_test.append(input)\n",
    "            y_test.append(expected_output)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
